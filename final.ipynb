{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\steam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\steam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\steam\\anaconda3\\lib\\site-packages\\tqdm\\std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, sys, time\n",
    "import requests\n",
    "from razdel import sentenize\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "from random import seed\n",
    "import spacy\n",
    "import ru_core_news_md\n",
    "\n",
    "\n",
    "import syllables\n",
    "import rusyllab\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from mlxtend.data import boston_housing_data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import holidays\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords');\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from tqdm.contrib import tzip\n",
    "from time import sleep\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as ltb\n",
    "import ast\n",
    "\n",
    "ru_holidays = holidays.country_holidays('RU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация признаков по дате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_day(datestring):\n",
    "    dt = datetime.strptime(datestring, '%Y-%m-%d %H:%M:%S')\n",
    "    return dt.weekday()\n",
    "\n",
    "def hour(datestring):\n",
    "    dt = datetime.strptime(datestring, '%Y-%m-%d %H:%M:%S')\n",
    "    return dt.hour\n",
    "\n",
    "def is_weekend(datestring):\n",
    "    dt = datetime.strptime(datestring, '%Y-%m-%d %H:%M:%S')\n",
    "    if dt.weekday() in [5,6]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_holiday(datestring):\n",
    "    dt = datetime.strptime(datestring, '%Y-%m-%d %H:%M:%S')\n",
    "    if dt.date() in ru_holidays:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_id(idd, ses):\n",
    "    return idd[:idd.find(ses)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение к статье и получение информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_info(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    paragraphs, words = 0, 0\n",
    "    token_paragraphs, count_sents = [], []\n",
    "    full_text = ''\n",
    "    for s in soup.find_all('p'):\n",
    "        if 'div' not in str(s) and any(c.isalpha() for c in s.text):\n",
    "            paragraphs += 1\n",
    "            tokens = word_tokenize(re.sub(r'[^а-яА-Яa-zA-z\\s]', '', s.text, re.I|re.A))\n",
    "            words += len(tokens)\n",
    "            token_paragraphs.append(tokens)\n",
    "            sentences = list(sentenize(s.text))\n",
    "            count_sents.append((len(sentences)))\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                full_text += sentence.text + ' '\n",
    "            \n",
    "\n",
    "    #Средняя длина предложения\n",
    "    avg_par_len = np.mean([len(tokens) for tokens in token_paragraphs]) if paragraphs !=0 else 0.0\n",
    "\n",
    "    #Количество предложений\n",
    "    sentence_count =  sum(count_sents) if paragraphs !=0 else 0\n",
    "    \n",
    "    #Доп категории\n",
    "    s = soup.find('div', {\"class\": \"article__header__info-block\"})\n",
    "    category = s.find('a', {\"class\": \"article__header__category\"}).contents[0]\n",
    "    \n",
    "    s = soup.find('div', {\"class\": \"article__tags__container\"})\n",
    "    \n",
    "    #Тэги\n",
    "    if s:\n",
    "        tags = [tag for tag in s.text.split('\\n') if tag]\n",
    "    else:\n",
    "        tags = []\n",
    "    \n",
    "    #-------------------------------------КОНТЕНТ----------------------------------------------\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    #Заголовочное изображение\n",
    "    if soup.find_all('div', {\"class\": \"article__main-image__wrap\"}):\n",
    "        title_image = 1\n",
    "    else:\n",
    "        title_image = 0\n",
    "        \n",
    "    #Количество видео\n",
    "    video_count = len(soup.find_all('div', {\"class\": \"article__inline-video js-insert-video-container\"}))\n",
    "    \n",
    "    #Количество изображений, помимо заголовочного изображения\n",
    "    gallery_count = len(soup.find_all('div', {\"class\": [\"gallery_vertical__image-wrap\",  \"gallery js-gallery\"]}))\n",
    "    pictures = soup.find_all('div', {\"class\": [\"article__picture\"]})\n",
    "    \n",
    "    if pictures:\n",
    "        pics_count, pics_margin  = 0, 0\n",
    "        for pic in pictures:\n",
    "            if str(pic).startswith('<div class=\"article__picture article__picture_margin\">'):\n",
    "                pics_margin += 1\n",
    "            else:\n",
    "                pics_count += 1\n",
    "        pics_count += int(pics_margin/2) \n",
    "    else:\n",
    "        pics_count = 0\n",
    "\n",
    "    image_count = pics_count + gallery_count\n",
    "    \n",
    "    \n",
    "    return category, tags, full_text, paragraphs, words, avg_par_len, sentence_count, title_image, video_count, image_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(word, morth=pymorphy2.MorphAnalyzer()):\n",
    "    \"Return a likely part of speech for the *word*.\"\"\"\n",
    "    return morth.parse(word)[0].tag.POS\n",
    "\n",
    "functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP', 'NPRO', 'ADVB'}  # function words\n",
    "\n",
    "def del_tags_from_text(title):\n",
    "    if title.find('\\n') != -1:\n",
    "        return title[:title.find('\\n')]\n",
    "    else:\n",
    "        return title\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in docs:\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        #doc = remove_accented_chars(doc)\n",
    "        doc = contractions.fix(doc)\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^а-яА-Я\\s]', '', doc, re.I|re.A) #0-9\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()  \n",
    "        norm_docs.append(doc)\n",
    "\n",
    "    return norm_docs\n",
    "\n",
    "\n",
    "def pos(word, morth=pymorphy2.MorphAnalyzer()):\n",
    "    \"Return a likely part of speech for the *word*.\"\"\"\n",
    "    return morth.parse(word)[0].tag.POS\n",
    "\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "morf = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_lem(sentence):\n",
    "\n",
    "    #lemmas = m.lemmatize(sentence)\n",
    "    #lemmas = [lemma for lemma in lemmas if (lemma not in stopwords and len(lemma) > 1)]\n",
    "    #lemmas = [word for word in lemmas if pos(word) not in functors_pos]\n",
    "    \n",
    "    #FIX\n",
    "    \n",
    "    #tokens = nlp(sentence)\n",
    "    #tokens = [token.lemma_ for token in tokens if (token not in stopwords and len(token) > 1)]\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    tokens = [word for word in tokens if pos(word) not in functors_pos]\n",
    "    \n",
    "    tokens = [morf.parse(token)[0].normal_form for token in tokens]\n",
    "    \n",
    "    #text = \" \".join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет важных слов названии статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_words(info_df, column, num_words = 100, count=19):\n",
    "    views_info = pd.DataFrame(info_df.groupby('words')[column].mean())\n",
    "    views_info = views_info.sort_values(by=[column], ascending=False).reset_index()\n",
    "    views_info['count'] = views_info.apply(lambda row: word_voc[row.words], axis = 1)\n",
    "    df_ = views_info.loc[views_info['count'] > count]\n",
    "    return df_, list(df_.words)[:num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_full(sentence):\n",
    "    sentence = del_tags_from_text(sentence)\n",
    "    preprocessed = pre_process_corpus([sentence])\n",
    "    tokens = get_lem(preprocessed[0])\n",
    "    #print(tokens)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in words_full:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def find_words_view(sentence):\n",
    "    sentence = del_tags_from_text(sentence)\n",
    "    preprocessed = pre_process_corpus([sentence])\n",
    "    tokens = get_lem(preprocessed[0])\n",
    "    #print(tokens)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in words_view:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применить функции к датафрейму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(df_train):\n",
    "    #df_train['authors'] = ordinal_encoder_ath.transform(np.array(df_train['authors']).reshape(-1, 1)).astype(int)\n",
    "    #df_train['category'] = ordinal_encoder_cat.transform(np.array(df_train['category']).reshape(-1, 1)).astype(int)\n",
    "    df_train['day'] = pd.to_datetime(df_train['publish_date']).dt.strftime(\"%d\").astype(int)\n",
    "    df_train['month'] = pd.to_datetime(df_train['publish_date']).dt.strftime(\"%m\").astype(int)\n",
    "    #df_train['weekday'] = df_train['publish_date'].apply(week_day)\n",
    "    df_train['hour'] = df_train['publish_date'].apply(hour)\n",
    "    df_train['weekend'] = df_train['publish_date'].apply(is_weekend)\n",
    "    df_train['holiday'] = df_train['publish_date'].apply(is_holiday)\n",
    "    #df_view, words_view = get_important_words(info, column='views', num_words = 100, count=10)\n",
    "    #df_full, words_full = get_important_words(info, column='full_reads_percent', num_words = 50, count=20)\n",
    "    df_train['full_affect'] = df_train['title'].apply(find_words_full)\n",
    "    df_train['view_affect'] = df_train['title'].apply(find_words_view)\n",
    "    #df_train = df_train.drop(columns=['publish_date', 'title'])\n",
    "    #print('ok')\n",
    "    df_train['id'] = df_train.apply(lambda df_train: get_id(df_train['document_id'], df_train['session']), axis=1)\n",
    "    df_train['URL'] = df_train['id'].apply(lambda x: 'http://rbc.ru/rbcfreenews/'  + x)\n",
    "    df_train = df_train.drop(columns=['id'])\n",
    "    df_train['cat'], df_train['tags'], df_train['text'], df_train['paragraphs'], df_train['words'], df_train['avg_par_len'], df_train['sentence_count'], df_train['title_image'], df_train['video_count'], df_train['image_count'] = zip(*df_train['URL'].progress_map(get_text_info))\n",
    "    \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слоги и буквы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllabs_letters(text):\n",
    "\n",
    "    rus_text = re.sub(r'[^а-яА-Я\\s]', '', text, re.I|re.A) #a-zA-z\n",
    "    eng_text = re.sub(r'[^a-zA-z\\s]', '', text, re.I|re.A)\n",
    "\n",
    "    if bool(eng_text.replace(\" \", \"\")):\n",
    "\n",
    "        eng_syb_count = 0\n",
    "        eng_syb_3_count = 0\n",
    "        \n",
    "        eng_tokens = eng_text.split()\n",
    "\n",
    "        for word in eng_tokens:\n",
    "            sbs = syllables.estimate(word)\n",
    "            eng_syb_count += sbs\n",
    "            \n",
    "            if sbs > 2:\n",
    "                eng_syb_3_count += 1\n",
    "        \n",
    "        eng_letters_count = len(eng_text.replace(\" \", \"\"))\n",
    "    else:\n",
    "        eng_syb_count = 0\n",
    "        eng_syb_3_count = 0\n",
    "        eng_letters_count = 0\n",
    "\n",
    "    if bool(rus_text.replace(\" \", \"\")):\n",
    "        \n",
    "        rus_syb_count = 0\n",
    "        rus_syb_3_count = 0\n",
    "        \n",
    "        rus_tokens = rus_text.split()\n",
    "        rus_tokens = [token for token in rus_tokens if token not in stopwords]\n",
    "        \n",
    "        for word in rus_tokens:\n",
    "            sbs = len(rusyllab.split_word(word))\n",
    "            rus_syb_count += sbs\n",
    "            \n",
    "            if sbs > 2:\n",
    "                rus_syb_3_count += 1           \n",
    "                \n",
    "        #syllab = rusyllab.split_words(rus_tokens)\n",
    "        #syllab = [syl for syl in syllab if bool(syl.replace(\" \", \"\"))]\n",
    "\n",
    "        #rus_syb_count = len(syllab)\n",
    "        rus_letters_count = len(rus_text.replace(\" \", \"\"))\n",
    "    else:\n",
    "        rus_syb_count = 0\n",
    "        rus_syb_3_count = 0\n",
    "        rus_letters_count = 0\n",
    "\n",
    "    syllabs = eng_syb_count + rus_syb_count\n",
    "    letters = eng_letters_count + rus_letters_count\n",
    "    syllabs_3 = eng_syb_3_count + rus_syb_3_count\n",
    "    \n",
    "    return syllabs, syllabs_3, letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Индексы читаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#где words – количество слов в тексте; \n",
    "#syllables – количество слогов в тексте; \n",
    "#sent – количество предложений в тексте\n",
    "def Flesh_Kincaid_Grade(syllabes, words, sent):\n",
    "\n",
    "    if words == 0 or sent == 0: return 0\n",
    "\n",
    "    n = 0.318 * (float(words) / sent) + 14.2 * (float(syllabes) / words) - 30.5\n",
    "\n",
    "    return n\n",
    "\n",
    "\n",
    "#letters –число букв, \n",
    "#words – число слов,\n",
    "#sent – число предложений\n",
    "def Coleman_Liau(letters, words, sent):\n",
    "\n",
    "    if words == 0: return 0\n",
    "\n",
    "    n = 0.055 * (letters * (100/ words)) - 0.35 * (sent * (100 / words)) - 20.33\n",
    "\n",
    "    return n\n",
    "\n",
    "#где psyl – число слов с 3-мя и более слогами; \n",
    "#sent – число предложений\n",
    "def SMOG(psyl, sent):\n",
    "    \n",
    "    if sent == 0: return 0\n",
    "    \n",
    "    n = 1.1 * np.sqrt((float(64.6) / sent) * psyl) + 0.05\n",
    "\n",
    "    return n\n",
    "\n",
    "\n",
    "#где n_letters – количество букв в тексте,\n",
    "#n_words – количество слов,\n",
    "#n_sent – количество предложений\n",
    "def calc_ARI_index(n_letters, n_words, n_sent):\n",
    "\n",
    "    if n_words == 0 or n_sent == 0: return 0\n",
    "\n",
    "    n = 6.26 * (float(n_letters) / n_words) + 0.2805 * (float(n_words) / n_sent) - 31.04\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difficulty(df_train):\n",
    "\n",
    "    df_train['FKG'] = df_train.progress_apply(lambda df_train: Flesh_Kincaid_Grade(df_train['syllabs'],\n",
    "                                                                                   df_train['words'],\n",
    "                                                                                   df_train['sentence_count']), axis=1)\n",
    "    \n",
    "\n",
    "    df_train['Coleman'] = df_train.progress_apply(lambda df_train: Coleman_Liau(df_train['letters'],\n",
    "                                                                                   df_train['words'],\n",
    "                                                                                   df_train['sentence_count']), axis=1)\n",
    "\n",
    "    df_train['SMOG'] = df_train.progress_apply(lambda df_train: SMOG(df_train['syllabs3'],\n",
    "                                                                        df_train['sentence_count']), axis=1)\n",
    "\n",
    "    \n",
    "    df_train['ARI'] = df_train.progress_apply(lambda df_train: calc_ARI_index(df_train['letters'],\n",
    "                                                                                   df_train['words'],\n",
    "                                                                                   df_train['sentence_count']), axis=1)\n",
    "    \n",
    "    return df_train\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_dataset_train.csv')\n",
    "df_test = pd.read_csv('test_dataset_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение важных слов в заголовке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:23<00:00, 296.17it/s]\n"
     ]
    }
   ],
   "source": [
    "new_titles = df_train['title'].apply(del_tags_from_text)\n",
    "new_titles = pre_process_corpus(new_titles)\n",
    "new_titles = [get_lem(title) for title in tqdm(new_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:01<00:00, 5481.96it/s]\n"
     ]
    }
   ],
   "source": [
    "words, views, full_reads_percent, depth = [], [], [], []\n",
    "\n",
    "for i in tqdm(range(len(new_titles))):\n",
    "    for word in new_titles[i]:\n",
    "        words.append(word)\n",
    "        views.append(df_train.views.iloc[i])\n",
    "        full_reads_percent.append(df_train.full_reads_percent.iloc[i])\n",
    "        depth.append(df_train.depth.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_voc = {}\n",
    "for token in set(words):\n",
    "    count = words.count(token)\n",
    "    word_voc[token] = count\n",
    "    \n",
    "voc = {'words':words, 'views':views, 'full_reads_percent': full_reads_percent, 'depth':depth}\n",
    "info = pd.DataFrame(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['count'] = info.apply(lambda row: word_voc[row.words], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>views</th>\n",
       "      <th>full_reads_percent</th>\n",
       "      <th>depth</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>европейский</td>\n",
       "      <td>20460</td>\n",
       "      <td>35.85</td>\n",
       "      <td>1.134</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>банк</td>\n",
       "      <td>20460</td>\n",
       "      <td>35.85</td>\n",
       "      <td>1.134</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>развитие</td>\n",
       "      <td>20460</td>\n",
       "      <td>35.85</td>\n",
       "      <td>1.134</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>приостановить</td>\n",
       "      <td>20460</td>\n",
       "      <td>35.85</td>\n",
       "      <td>1.134</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>доступ</td>\n",
       "      <td>20460</td>\n",
       "      <td>35.85</td>\n",
       "      <td>1.134</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  views  full_reads_percent  depth  count\n",
       "0    европейский  20460               35.85  1.134     15\n",
       "1           банк  20460               35.85  1.134     80\n",
       "2       развитие  20460               35.85  1.134     14\n",
       "3  приостановить  20460               35.85  1.134     49\n",
       "4         доступ  20460               35.85  1.134     21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view, words_view = get_important_words(info, column='views', num_words = 100, count=10)\n",
    "df_full, words_full = get_important_words(info, column='full_reads_percent', num_words = 50, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final = get_final_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = get_final_df(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применение tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_train_final['text']\n",
    "titles = pre_process_corpus(titles)\n",
    "titles = [get_lem(title) for title in tqdm(titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=200)\n",
    "X = vectorizer.fit_transform(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train_final, tf_df], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(titles)\n",
    "tf_df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test_final, tf_df], axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применить индексы читаемости и сохранить файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = text_difficulty(df_train)\n",
    "df_test = text_difficulty(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['title', 'text', 'URL'])\n",
    "df_test = df_test.drop(columns=['title', 'text', 'URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('full_last_train.csv')\n",
    "df_test.to_csv('full_last_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
